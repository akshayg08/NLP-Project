{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the sorted_MX and sorted_MY\n",
    "def normalize(X):\n",
    "\t# length normalization\n",
    "\tx = X/np.linalg.norm(X, axis = 1).reshape(-1, 1)\n",
    "\t# mean centering each dimension\n",
    "\tx = x - x.mean(axis = 0)\n",
    "\t# length normalizing the mean centered data\n",
    "\tx = x/np.linalg.norm(x, axis = 1).reshape(-1, 1)\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data\n",
    "\n",
    "For this to work, we need the:\n",
    "1. Embedding in Hindi, English\n",
    "2. Vocabulary used to generate the Embedding\n",
    "3. A hindi-english dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_idx2word_dict(vocab):\n",
    "\tidx2word = {}\n",
    "\tfor i in vocab:\n",
    "\t\tidx2word[vocab[i]] = i\n",
    "\n",
    "\treturn idx2word\n",
    "\n",
    "# Building Word Pairs\n",
    "with open('../data/en-hi.txt') as f:\n",
    "    word_pairs = []\n",
    "    for line in f:\n",
    "        ls = line.split('\\t')\n",
    "        ls[1] = ls[1][:-1]\n",
    "        word_pairs.append(ls)\n",
    "word_pairs = np.array(word_pairs)\n",
    "\n",
    "\n",
    "# Building vocab-index maps\n",
    "hindi_vocab = pickle.load(open(\"./hindi_vocab.pkl\", \"rb\"))\n",
    "english_vocab = pickle.load(open(\"./english_vocab.pkl\", \"rb\"))\n",
    "hindi_idx2word = create_idx2word_dict(hindi_vocab)\n",
    "english_idx2word = create_idx2word_dict(english_vocab)\n",
    "\n",
    "# with open('ywy_without_csls_without_improvement.pkl', 'rb') as f:\n",
    "# with open('ywy_with_csls_without_improvement.pkl', 'rb') as f:\n",
    "# with open('ywy_without_csls_with_improvement.pkl', 'rb') as f:\n",
    "with open('ywy_with_csls_with_improvement.pkl', 'rb') as f:\n",
    "    embed_en = pickle.load(f)\n",
    "\n",
    "# with open('xwx_without_csls_without_improvement.pkl', 'rb') as f:\n",
    "# with open('xwx_with_csls_without_improvement.pkl', 'rb') as f:\n",
    "# with open('xwx_without_csls_with_improvement.pkl', 'rb') as f:\n",
    "with open('xwx_with_csls_with_improvement.pkl', 'rb') as f:\n",
    "    embed_hi = pickle.load(f)\n",
    "    \n",
    "embed_en = normalize(embed_en)\n",
    "embed_hi = normalize(embed_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['and', 'और'],\n",
       "       ['was', 'था'],\n",
       "       ['was', 'थी'],\n",
       "       ...,\n",
       "       ['pattabhi', 'पट्टाभि'],\n",
       "       ['golmud', 'गोलमुद'],\n",
       "       ['folliculitis', 'folliculitis']], dtype='<U27')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsic Measures\n",
    "\n",
    "## Translation Similarity test\n",
    "Measuring the average cosine similarity between the embeddings of translation word pairs in English and Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage:  0.15125192956751524\n",
      "Accuracy:  0.4279327308203106\n"
     ]
    }
   ],
   "source": [
    "cos_all = []\n",
    "for pair in word_pairs:\n",
    "    \n",
    "    if (pair[0] not in english_vocab) or (pair[1] not in hindi_vocab):\n",
    "        continue\n",
    "    \n",
    "    cos_all.append(abs(np.dot(\n",
    "        embed_en[english_vocab[pair[0]]],\n",
    "        embed_hi[hindi_vocab[pair[1]]],\n",
    "    )))\n",
    "    \n",
    "cos_all = np.array(cos_all)\n",
    "print(\"Coverage: \", len(cos_all)/len(word_pairs))\n",
    "print(\"Accuracy: \", np.mean(cos_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-74.20290503293218"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(\n",
    "        embed_en[english_vocab[word_pairs[0][0]]],\n",
    "        embed_hi[hindi_vocab[word_pairs[0][1]]],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
